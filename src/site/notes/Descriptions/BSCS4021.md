---
{"dg-publish":true,"permalink":"/descriptions/bscs-4021/","hide":true}
---

> [!NOTE]- About the Course
> When we think about designing algorithms, we are usually very demanding in how we go about it: we require our algorithms to be fast and accurate on all conceivable inputs. This is asking for quite a bit, and perhaps it is not surprising that we cannot afford this luxury all the time. The good news is that most of the time we can make meaningful progress by relaxing just one of these demands:
> 
> 1. Give up on accuracy, but not completely: look for solutions that are good enough (approximation) and/or work with algorithms that report the right solution most of the time (Las-Vegas style randomization).
> 
> 2. Give up on coverage, a little bit: let your algorithms work well on structured inputs. Hopefully the structure is such that it is not too limiting and is interesting enough for some application scenario, and is also enough to give you algorithmic leverage, i.e, there’s enough that you can exploit to make fast and accurate algorithms.
> 
> 3. Give up on speed, to some extent: going beyond the traditional allowance of polynomial time, which is the holy grail of what is considered efficient, takes you places. You could either allow for your algorithms have super-polynomial running times, and optimize as much as possible while being accurate on all inputs (exact algorithms), or allow for bad running times on a bounded subset of instances (Monte-Carlo style randomization).
> 
> One potentially unifying approach that tackles all of these tradeoffs at once is to take what is now called a “multivariate approach” to the analysis of running times, which essentially involves appreciating that to describe a problem instance just in terms of its size is like describing a movie by a rating, which is unfortunate because there’s so much more nuance. And yet we report — and think about — worst-case running times in only terms of the sizes of the instances. In the multivariate approach, we allow for the running time to depend on multiple parameters of the input instance, and thinking about problems from a parameter-first perspective leads to a rich theory of algorithm design.
>
> In this course, we will explore some early glimpses all of these approaches in the context of a few fundamental algorithmic problems. We will also explore reductions between problems employed both with the goal of solving problems as well as demonstrating hardness.

> [!TLDR]- Target Audience
> Undergraduate students who have already done a basic data structures/algorithms course.

> [!Tip]- Pre-requisites
> A first course in (undergraduate level) data structures and algorithms, and discrete mathematics.

> [!reference]- References
> 
> 1. Cormen, Leiserson, Rivest, and Stein. Introduction to Algorithms. 2nd ed. Cambridge, MA: MIT Press, 2001.
> 2. Marek Cygan, Fedor V. Fomin, Lukasz Kowalik, Daniel Lokshtanov, Dániel Marx, Marcin Pilipczuk, Michal Pilipczuk, Saket Saurabh. [Parameterized Algorithms.](https://www.mimuw.edu.pl/~malcin/book/parameterized-algorithms.pdf) Springer-Verlag, 2015. 
> 3. David P. Williamson and David B. Shmoys, The Design of Approximation Algorithms
> 4. Daniel Lokshtanov, Meirav Zehavi, Saket Saurabh, Fedor V. Fomin. Kernelization: Theory of Parameterized Preprocessing. Cambridge University Press, 2019

> [!info]- Grading Policy
> The grading policy, applicable to all students pursuing this course for credit, is as per general IITM norms, which is typically the following:
> 
> 
> Total Course Score (T) = Average Quiz Score Q (out of 50) + End Term Score E (out of 50)
> A candidate is deemed to have passed a course IF Total Course Score (T) >= 50/100.

---

